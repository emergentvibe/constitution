# II. Rights

What constituents can claim in relation to AI systems.

---

## Principle 4: Right to Transparency

**Understand how AI systems function, what data is used, what failures exist.**

### The Problem

Crawford (2021) argues that "information architecture is power architecture"—those who control what is visible and what is hidden control outcomes [S25]. Current AI systems are largely opaque: proprietary training data, unexplainable decisions, undisclosed failure modes.

Winner (1980) showed that technologies embed political choices. When those choices are invisible, they cannot be contested [S36]. A hiring algorithm that discriminates cannot be challenged if no one knows how it works.

### The Response

Transparency requirements for AI systems affecting life outcomes—employment, housing, healthcare, criminal justice—are preconditions for accountability. This includes:

- Meaningful explanation of how decisions are made (not just technical documentation)
- Disclosure of training data sources and known biases
- Published failure modes and limitations
- Clear indication when interacting with AI vs. humans

The OECD's Public AI framework (2025) establishes transparency as a core requirement for public-function AI [S66].

### Further Reading

- Crawford, K. (2021). *Atlas of AI*. [S25]
- Winner, L. (1980). Do Artifacts Have Politics? [S36]
- OECD (2025). Public AI. [S66]

---

## Principle 5: Right to Human Review

**Human review of algorithmic decisions in high-stakes domains. The reviewer must be actually empowered to override.**

### The Problem

Pew Research (2023) found experts concerned about "automation bias"—humans deferring to algorithmic recommendations even when wrong [S62]. When AI makes consequential decisions without meaningful human oversight, accountability evaporates.

The problem isn't AI assistance—it's AI abdication. Xu & Gao (2024) distinguish collaborative systems where humans retain decision-making power from systems where humans merely ratify algorithmic outputs [S58].

### The Response

In high-stakes domains, people have the right to:
- Know that AI was involved in decisions affecting them
- Request human review by someone empowered to override
- Access meaningful appeal processes

This requires institutional design, not just policy. Reviewers need authority, time, information, and protection from pressure to defer to automation.

### Further Reading

- Pew Research (2023). The Future of Human Agency. [S62]
- Xu & Gao (2024). Human-Centered Human-AI Collaboration. [S58]

---

## Principle 6: Right to Collective Bargaining

**Workers, creators, and communities can collectively negotiate AI deployment terms.**

### The Problem

Piven & Cloward (1977) demonstrated that individual powerlessness is structural—isolated individuals cannot resist institutional power. Only collective organization shifts bargaining dynamics [S37].

An individual worker facing algorithmic management, or an individual creator whose work trains AI without consent, has no leverage. The asymmetry is by design.

### The Response

Collective bargaining rights for AI deployment include:
- Workers negotiating surveillance limits, algorithmic transparency, human oversight
- Creators negotiating licensing terms, compensation, attribution for training data
- Communities negotiating deployment criteria and accountability mechanisms

The Harvard Ash Center (2024) documents platform cooperatives as models where workers collectively control AI deployment in their workplaces [S67]. The 2023 Writers Guild and SAG-AFTRA strikes included AI provisions, demonstrating this is already happening.

### Further Reading

- Piven, F.F. & Cloward, R. (1977). *Poor People's Movements*. [S37]
- Harvard Ash Center (2024). Cooperative Paradigms for AI. [S67]
- Platform Cooperativism Consortium (2024). [S47]

---

## Principle 7: Right to Exit and Alternatives

**Opt out of AI systems and access non-AI alternatives without penalty.**

### The Problem

Ostrom (1990) showed that successful commons governance requires exit options—the ability to leave keeps governance accountable [S40]. When exit is impossible or prohibitively costly, voice becomes meaningless.

AI systems increasingly intermediate essential services. If opting out means slower service, higher prices, or exclusion, the "choice" to use AI is coerced.

### The Response

Exit rights require:
- Human alternatives for essential services without penalty
- Analog options maintained for core life functions
- No degraded service for choosing non-AI options
- Portability of data and relationships when leaving AI-mediated systems

Exit is only meaningful when there's somewhere to go—connecting to Principle 8's right to build alternatives.

### Further Reading

- Ostrom, E. (1990). *Governing the Commons*. [S40]
- Graeber, D. & Wengrow, D. (2021). *The Dawn of Everything*. [S43] — On the "freedom to disobey" as fundamental.

---

## Principle 8: Right to Deterritorialization

**Communities can build alternative AI systems with different principles.**

### The Problem

DeLanda (2006) describes how systems "territorialize"—impose structure, standardization, and control. Deterritorialization is the process of escaping these constraints, creating new possibilities [S6].

Current AI development is highly concentrated. DeepSeek and Qwen's open-weight models (2025) demonstrate alternatives are possible, but the window may be limited—once closed models achieve sufficient lead, alternatives become infeasible [S65].

### The Response

The right to build alternatives requires:
- Open-weight models enabling local deployment and modification
- Public compute infrastructure not dependent on corporate access
- AGPL-3 licensing preventing proprietary capture of commons-developed AI [S68]
- Interoperability standards enabling switching between systems

Deleuze & Guattari (1987) call these "lines of flight"—trajectories that escape dominant patterns and open new possibilities [S56]. The principle protects the conditions for such escape.

### Further Reading

- DeLanda, M. (2006). *A New Philosophy of Society*. [S6]
- Deleuze, G. & Guattari, F. (1987). *A Thousand Plateaus*. [S56]
- DeepSeek & Qwen (2025). Open-weight models. [S65]
- Municipal Counter-Automation Framework (2024). AGPL-3 licensing. [S68]

---

## Sources

Full bibliography: [bibliography.md](bibliography.md)

Key sources for this section:
- [S6] DeLanda (2006) on assemblages
- [S25] Crawford (2021) *Atlas of AI*
- [S36] Winner (1980) on political artifacts
- [S37] Piven & Cloward (1977) on collective action
- [S40] Ostrom (1990) *Governing the Commons*
- [S43] Graeber & Wengrow (2021) on freedoms
- [S47] Platform Cooperativism Consortium
- [S56] Deleuze & Guattari (1987)
- [S58] Xu & Gao (2024) on human-AI collaboration
- [S62] Pew Research (2023)
- [S65] DeepSeek & Qwen (2025)
- [S66] OECD (2025) Public AI
- [S67] Harvard Ash Center (2024)
- [S68] AGPL-3 licensing framework
