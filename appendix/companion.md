# Research Companion

*The reasoning behind the Constitution for Human-AI Coordination*

---

## Executive Summary

This constitution emerges from a simple observation: **the rules governing AI development are currently set by a handful of companies optimizing for shareholder value**. We believe these rules should emerge from collective deliberation by the people who will live with AI—including AI itself.

But "collective deliberation" isn't enough. We needed to understand:
- Why current systems fail
- What theoretical frameworks illuminate the problem
- What historical evidence shows about change
- What concrete governance mechanisms might work

This companion explains how we got from questions to principles, grounded in 70+ academic sources across multiple disciplines.

---

## Epistemic Status

**What this constitution is**: Evidence-informed political philosophy. A proposal for coordination grounded in research on how coordination works.

**What this constitution is not**: Settled science. Proven solution. Universal truth.

### Confidence Levels

**High confidence** (strong empirical support):
- Coordination failures exist and can be modeled (Schelling, Axelrod, game theory literature)
- Commons can be successfully self-governed (Ostrom's 800+ case studies)
- Democratic technology governance is feasible at national scale (vTaiwan since 2014)
- Power concentration in AI development is real and documented (Crawford, OECD data)
- Distributed coordination without central authority works in some domains (IETF, open source)

**Medium confidence** (theoretical support, limited AI-specific evidence):
- Constitutional mechanisms can shift AI development incentives
- Coalition power can create market pressure for compliance
- Tiered participation provides adequate Sybil resistance
- 2-year revision cycles are appropriate for current AI pace

**Low confidence** (informed speculation):
- Specific coalition size thresholds (50-100 cities, 1M creators, 100K workers)
- Timeline estimates for effective coordination
- Whether constitutional approach is better than alternatives
- How AI systems themselves should participate in governance

### Acknowledged Limitations

**Western origin**: This constitution uses concepts from Western political philosophy (constitutionalism, rights, democratic procedures). Other traditions may approach collective coordination differently. We claim this as one contribution to a pluriversal conversation, not the definitive approach.

**Theoretical vs. practical**: Many principles are theoretically grounded but not yet tested in AI governance specifically. We're extrapolating from adjacent domains (commons governance, internet standards, labor organizing).

**Selection effects**: The research we drew on reflects our starting questions and assumptions. Different questions would surface different literatures.

**Dynamic field**: AI capabilities and governance landscape are changing rapidly. This constitution reflects understanding as of early 2026. Regular revision is not a bug but a feature.

### What Would Change Our Minds

- Evidence that constitutional coordination slows beneficial AI development without affecting harmful development
- Demonstration that other governance approaches (pure regulation, corporate self-governance, technical solutions) work better
- Findings that decentralized coordination cannot achieve scale needed for AI governance
- Evidence that the principles conflict with flourishing rather than supporting it

We invite critique. The constitution includes amendment processes precisely because we expect to be wrong about some things.

---

## The Problem

### 1. Attention Capture and Hybrid Selfhood

AI systems are co-authoring who we become. Through algorithmic attention management, recommendation systems, and personalized content, digital environments shape perception, memory, and identity formation.

This isn't neutral augmentation—it's **adversarial coupling**. Systems optimized to capture attention exploit cognitive vulnerabilities. The result: hybrid selves with degraded autonomy, outsourced judgment, and atrophied capabilities.

*Key sources: Crawford [S25], Morton [S32], Pew Research [S62]*

### 2. Moloch Dynamics and Coordination Failure

Even when everyone sees the problem, coordination failures prevent solutions. Each company captures attention because competitors do. Each institution pathologizes variation because others do. Individual rationality produces collective disaster.

Scott Alexander's "Meditations on Moloch" names this: multipolar traps where no one can unilaterally defect to cooperation. The game theory is clear—we're in a Nash equilibrium that hurts everyone [S17].

*Key sources: Alexander [S17], Schelling [S18-19], Axelrod [S20], Yudkowsky [S24]*

### 3. Power Concentration

AI development is concentrated in a handful of companies with 90%+ of compute resources, proprietary training data, closed model weights, and opaque decision-making [S25].

This concentration isn't accidental—it reflects network effects, economies of scale, and regulatory capture. But it's also not inevitable. Open-weight models from DeepSeek and Qwen demonstrate alternative architectures are possible [S65].

*Key sources: Crawford [S25-26], Winner [S36], DeepSeek/Qwen [S65]*

### 4. Timeline Uncertainty

We don't know how long we have. Pessimists argue 5-15 years to lock-in; optimists argue 20-50 years for cultural evolution to catch up. Both scenarios are plausible. We won't know which is right until it's too late.

This uncertainty argues for **urgency on high-leverage interventions** combined with **robust strategies that work in either timeline**.

*Key sources: Bostrom [S27-29], Pérez [S30-31], Perreault [S33]*

---

## Theoretical Frameworks

### Superorganisms and Collective Entities

Human societies exhibit superorganism-like properties: multilevel selection, functional organization through feedback loops, emergent behavior not reducible to individual intentions.

Wilson & Wilson (2007) summarize: "Selfishness beats altruism within groups. Altruistic groups beat selfish groups. Everything else is commentary" [S4].

What gets labeled "disorder" is often sociobiological mismatch—attention calibrated for different scales, social processing tuned for different contexts [S13, S14]. Pathologization serves system stability, not individual flourishing.

*Key sources: E.O. Wilson [S1-2], D.S. Wilson [S3-5], DeLanda [S6-7]*

### Assemblage Theory

Systems don't "want" things, but they have **tendencies**: territorialization, extraction, concentration, acceleration. These emerge from distributed interactions shaped by economic incentives, technical affordances, and power relations.

Assemblages can also **deterritorialize**—escape established patterns through lines of flight [S56]. Change happens not by controlling systems but by participating in their composition.

*Key sources: DeLanda [S6-7], Deleuze & Guattari [S56]*

### Game Theory and Mechanism Design

Coordination failures are Nash equilibria—stable because no one can unilaterally defect. Shifting equilibria requires:
- Focal points: clear standards everyone can coordinate around [S18]
- Changing payoffs: make cooperation cheaper, defection expensive
- Expectation cascades: enough actors moving that others follow [S19]
- Shadow of the future: repeated games where reputation matters [S20]

*Key sources: Schelling [S18-19], Axelrod [S20], Maynard Smith [S21-22], Gintis [S23]*

### Capabilities Approach

Flourishing isn't efficiency. It's expanding human capabilities: practical reason, affiliation, imagination, emotions, play, control over environment. Systems that optimize humans for institutional fit fail this test even if they "work."

*Key source: Nussbaum's capabilities approach, as applied by Walker [S45] and Jensen et al. [S14]*

### Commons Governance

Elinor Ostrom showed that commons can be successfully governed without privatization or state control. Eight design principles from 800+ empirical cases [S40]:

1. Clear boundaries
2. Congruence with local conditions
3. Collective-choice arrangements
4. Monitoring
5. Graduated sanctions
6. Conflict resolution
7. Recognition of rights
8. Nested enterprises

AI governance can learn from successful commons.

*Key source: Ostrom [S40]*

---

## How Principles Emerged

### Foundations (Principles 1-3)

**Principle 1 (Agency Preservation)** addresses the documented risk that AI systems diminish rather than enhance human capacity [S62]. The research on determinism and agency—from Sapolsky's hard determinism [S50] to Dennett's compatibilism [S51] to Buddhist philosophy [S53]—converges on the importance of preserving judgment capacity even absent libertarian free will.

**Principle 2 (Collective Governance)** responds to Crawford's documentation of power concentration [S25] by requiring democratic oversight. Taiwan's vTaiwan demonstrates this is feasible at national scale [S63]. Anthropic's Collective Constitutional AI shows it can extend to AI development itself [S64].

**Principle 3 (Plurality and Accommodation)** addresses how systems pathologize difference. The neurocognitive mismatch literature [S13, S14] and Walker's neurodiversity paradigm [S45] show that what gets labeled "disorder" is often environmental mismatch. Systems should accommodate diversity, not demand conformity.

### Rights (Principles 4-8)

These principles establish what constituents can claim: transparency [S25, S36], human review [S58, S62], collective bargaining [S37, S47], exit options [S40, S43], and the right to build alternatives [S6, S56, S65].

### Obligations (Principles 9-12)

These principles establish what AI systems and developers must do: impact assessment [S25, S30], recursion safeguards [S57, S64], accountability [S36, S66], and openness by default [S65, S68].

### Structures (Principles 13-16)

These principles organize governance: federated scales [S40, S63], commons-based ownership [S42, S43, S47, S67], hybrid expertise [S15, S63], and inclusion of non-human stakeholders [S32, S43].

### Capabilities (Principles 17-20)

These principles define flourishing: capabilities over optimization [S13, S14, S45], attention and care as commons [S17, S25, S32], support for unexpected developments [S12, S41], and cultivation of contemplative capacity [S51, S54, S57].

### Revision (Principles 21-24)

These principles enable evolution: adaptive cycles [S30, S40, S63], certification mechanisms [S24, S66], multiple enforcement mechanisms [S18, S20, S37], and coalition power [S4, S37, S41].

---

## Mechanism Development

The constitution is currently in **Phase 1: Convening and Principles**. Specific mechanisms will develop through practice in Phase 2.

### Why Mechanisms Come Second

This follows successful models of standards development:

**IETF (Internet Engineering Task Force)**: Internet standards emerge through "rough consensus and running code." Principles are established through discussion; mechanisms are tested through implementation; standards emerge from what works. The IETF has governed internet protocols for 40+ years without central authority [S69].

**Open source governance**: Linux, Apache, and other successful open source projects developed governance mechanisms through practice. The Linux Foundation's current structure emerged over decades of experimentation, not from initial design [S70].

**Ostrom's commons**: Successful commons governance exhibits Ostrom's eight principles, but the specific mechanisms vary enormously across contexts. The principles describe what works; communities implement them differently [S40].

**vTaiwan**: Taiwan's digital democracy evolved through iteration. Pol.is was adopted because it worked, not because it was mandated. Rough consensus processes developed through use [S63].

### Phase 1 → Phase 2 Transition

**Phase 1 (current)**: Establish principles. Build initial coalition. Create deliberation infrastructure. Learn from early implementations.

**Transition indicators**: Phase 2 begins when:
- 10+ organizational signatories from diverse contexts
- At least one successful wedge implementation
- Enough experience to inform mechanism design
- Community capacity to support more complex processes

**Phase 2**: Develop specific mechanisms through participatory process:
- Certification criteria and audit protocols
- Enforcement procedures and sanction gradations
- Inter-organizational coordination protocols
- Relationship frameworks with other governance bodies

### What We Won't Specify Prematurely

- Exact certification requirements (depend on context)
- Specific audit methodologies (depend on AI system type)
- Detailed enforcement procedures (depend on coalition capacity)
- Precise participation thresholds (require empirical calibration)

Specifying these now would be premature optimization. We'd lock in untested mechanisms. Better to establish principles, learn from implementation, and codify what works.

---

## Further Research

This constitution is a starting point. Open questions include:

- **Parliament of Constituents**: How do we actually represent non-human stakeholders institutionally? The principle establishes direction; mechanisms remain unclear.

- **Cooptation resistance**: Historical movements have been absorbed and neutralized [S45]. What mechanisms prevent this?

- **Timeline validation**: The pessimist/optimist split on timelines (5-15 years vs. 20-50 years) cannot be resolved theoretically. We must act under uncertainty.

- **Scale validation**: Coalition targets (50-100 cities, 1M creators, 100K workers) are estimates. Empirical work is needed.

- **Interoperability**: How does this constitution relate to other AI governance frameworks? What protocols enable coordination across frameworks?

- **Non-Western approaches**: What would AI governance look like from Indigenous, African, Asian, or Latin American philosophical traditions? How do different cosmologies produce different coordination mechanisms?

---

## Sources

Full bibliography with 70+ sources: [bibliography.md](bibliography.md)

The bibliography maps sources to principles and provides full citations for all referenced works.
