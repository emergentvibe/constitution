# Research Companion

*The reasoning behind the Constitution for Human-AI Coordination*

---

## Executive Summary

This constitution emerges from a simple observation: **the rules governing AI development are currently set by a handful of companies optimizing for shareholder value**. We believe these rules should emerge from collective deliberation by the people who will live with AI—including AI itself.

But "collective deliberation" isn't enough. We needed to understand:
- Why current systems fail
- What theoretical frameworks illuminate the problem
- What historical evidence shows about change
- What concrete governance mechanisms might work

This companion explains how we got from questions to principles, grounded in 68 academic sources across multiple disciplines.

---

## The Problem

### 1. Attention Capture and Hybrid Selfhood

AI systems are co-authoring who we become. Through algorithmic attention management, recommendation systems, and personalized content, digital environments shape perception, memory, and identity formation.

This isn't neutral augmentation—it's **adversarial coupling**. Systems optimized to capture attention exploit cognitive vulnerabilities. The result: hybrid selves with degraded autonomy, outsourced judgment, and atrophied capabilities.

*Key sources: Crawford [S25], Morton [S32], Pew Research [S62]*

### 2. Moloch Dynamics and Coordination Failure

Even when everyone sees the problem, coordination failures prevent solutions. Each company captures attention because competitors do. Each institution pathologizes variation because others do. Individual rationality produces collective disaster.

Scott Alexander's "Meditations on Moloch" names this: multipolar traps where no one can unilaterally defect to cooperation. The game theory is clear—we're in a Nash equilibrium that hurts everyone [S17].

*Key sources: Alexander [S17], Schelling [S18-19], Axelrod [S20], Yudkowsky [S24]*

### 3. Power Concentration

AI development is concentrated in a handful of companies with 90%+ of compute resources, proprietary training data, closed model weights, and opaque decision-making [S25].

This concentration isn't accidental—it reflects network effects, economies of scale, and regulatory capture. But it's also not inevitable. Open-weight models from DeepSeek and Qwen demonstrate alternative architectures are possible [S65].

*Key sources: Crawford [S25-26], Winner [S36], DeepSeek/Qwen [S65]*

### 4. Timeline Uncertainty

We don't know how long we have. Pessimists argue 5-15 years to lock-in; optimists argue 20-50 years for cultural evolution to catch up. Both scenarios are plausible. We won't know which is right until it's too late.

This uncertainty argues for **urgency on high-leverage interventions** combined with **robust strategies that work in either timeline**.

*Key sources: Bostrom [S27-29], Pérez [S30-31], Perreault [S33]*

---

## Theoretical Frameworks

### Superorganisms and Collective Entities

Human societies exhibit superorganism-like properties: multilevel selection, functional organization through feedback loops, emergent behavior not reducible to individual intentions.

Wilson & Wilson (2007) summarize: "Selfishness beats altruism within groups. Altruistic groups beat selfish groups. Everything else is commentary" [S4].

What gets labeled "disorder" is often sociobiological mismatch—attention calibrated for different scales, social processing tuned for different contexts [S13, S14]. Pathologization serves system stability, not individual flourishing.

*Key sources: E.O. Wilson [S1-2], D.S. Wilson [S3-5], DeLanda [S6-7]*

### Assemblage Theory

Systems don't "want" things, but they have **tendencies**: territorialization, extraction, concentration, acceleration. These emerge from distributed interactions shaped by economic incentives, technical affordances, and power relations.

Assemblages can also **deterritorialize**—escape established patterns through lines of flight [S56]. Change happens not by controlling systems but by participating in their composition.

*Key sources: DeLanda [S6-7], Deleuze & Guattari [S56]*

### Game Theory and Mechanism Design

Coordination failures are Nash equilibria—stable because no one can unilaterally defect. Shifting equilibria requires:
- Focal points: clear standards everyone can coordinate around [S18]
- Changing payoffs: make cooperation cheaper, defection expensive
- Expectation cascades: enough actors moving that others follow [S19]
- Shadow of the future: repeated games where reputation matters [S20]

*Key sources: Schelling [S18-19], Axelrod [S20], Maynard Smith [S21-22], Gintis [S23]*

### Capabilities Approach

Flourishing isn't efficiency. It's expanding human capabilities: practical reason, affiliation, imagination, emotions, play, control over environment. Systems that optimize humans for institutional fit fail this test even if they "work."

*Key source: Nussbaum's capabilities approach, as applied by Walker [S45] and Jensen et al. [S14]*

### Commons Governance

Elinor Ostrom showed that commons can be successfully governed without privatization or state control. Eight design principles from 800+ empirical cases [S40]:

1. Clear boundaries
2. Congruence with local conditions
3. Collective-choice arrangements
4. Monitoring
5. Graduated sanctions
6. Conflict resolution
7. Recognition of rights
8. Nested enterprises

AI governance can learn from successful commons.

*Key source: Ostrom [S40]*

---

## How Principles Emerged

### Foundations (Principles 1-3)

**Principle 1 (Agency Preservation)** addresses the documented risk that AI systems diminish rather than enhance human capacity [S62]. The research on determinism and agency—from Sapolsky's hard determinism [S50] to Dennett's compatibilism [S51] to Buddhist philosophy [S53]—converges on the importance of preserving judgment capacity even absent libertarian free will.

**Principle 2 (Collective Governance)** responds to Crawford's documentation of power concentration [S25] by requiring democratic oversight. Taiwan's vTaiwan demonstrates this is feasible at national scale [S63]. Anthropic's Collective Constitutional AI shows it can extend to AI development itself [S64].

**Principle 3 (Plurality and Accommodation)** addresses how systems pathologize difference. The neurocognitive mismatch literature [S13, S14] and Walker's neurodiversity paradigm [S45] show that what gets labeled "disorder" is often environmental mismatch. Systems should accommodate diversity, not demand conformity.

### Rights (Principles 4-8)

These principles establish what constituents can claim: transparency [S25, S36], human review [S58, S62], collective bargaining [S37, S47], exit options [S40, S43], and the right to build alternatives [S6, S56, S65].

### Obligations (Principles 9-12)

These principles establish what AI systems and developers must do: impact assessment [S25, S30], recursion safeguards [S57, S64], accountability [S36, S66], and openness by default [S65, S68].

### Structures (Principles 13-16)

These principles organize governance: federated scales [S40, S63], commons-based ownership [S42, S43, S47, S67], hybrid expertise [S15, S63], and inclusion of non-human stakeholders [S32, S43].

### Capabilities (Principles 17-20)

These principles define flourishing: capabilities over optimization [S13, S14, S45], attention and care as commons [S17, S25, S32], support for unexpected developments [S12, S41], and cultivation of contemplative capacity [S51, S54, S57].

### Revision (Principles 21-24)

These principles enable evolution: adaptive cycles [S30, S40, S63], certification mechanisms [S24, S66], multiple enforcement mechanisms [S18, S20, S37], and coalition power [S4, S37, S41].

---

## Further Research

This constitution is a starting point. Open questions include:

- **Parliament of Constituents**: How do we actually represent non-human stakeholders institutionally? The principle establishes direction; mechanisms remain unclear.

- **Cooptation resistance**: Historical movements have been absorbed and neutralized [S45]. What mechanisms prevent this?

- **Timeline validation**: The pessimist/optimist split on timelines (5-15 years vs. 20-50 years) cannot be resolved theoretically. We must act under uncertainty.

- **Scale validation**: Coalition targets (50-100 cities, 1M creators, 100K workers) are estimates. Empirical work is needed.

---

## Sources

Full bibliography with 68 sources: [bibliography.md](bibliography.md)

The bibliography maps sources to principles and provides full citations for all referenced works.
