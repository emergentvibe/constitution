# V. Capabilities

What flourishing requires.

---

## Principle 17: Flourishing Over Optimization

**Expand capabilities for valuable lives—practical reason, affiliation, imagination, emotion, play, control over environment. Don't optimize entities for system efficiency.**

### The Problem

Systems tend to optimize their components for system-level efficiency. This is fine for machines, dangerous for beings with intrinsic value. AI systems that make workers faster but humans smaller fail to serve genuine flourishing.

The neurocognitive mismatch literature [S13, S14] shows how environments optimized for "normal" functioning pathologize natural human variation. Jensen et al. (2021) demonstrate that ADHD traits may have been adaptive in ancestral environments but create dysfunction when forced into modern structured settings [S14].

### The Response

Drawing on capabilities approaches (Nussbaum, as applied by Walker [S45]):

**Flourishing requires expanding human capacity for:**
- Practical reason and planning
- Affiliation and social connection
- Imagination and creativity
- Emotional range and expression
- Play and leisure
- Control over one's environment

**AI systems should be evaluated by whether they expand or contract these capacities.**

A system that increases productivity but diminishes imagination, or that enhances efficiency but erodes affiliation, fails this test—even if it "works" by narrow metrics.

### Further Reading

- Jensen, P.S. et al. (2021). Evolutionary thinking and ADHD. [S14]
- Walker, N. (2021). *Neuroqueer Heresies*. [S45]
- Neurocognitive Mismatch Theory (2025). [S13]

---

## Principle 18: Care and Attention as Commons

**Protect attention, care, and relational capacity. Don't extract them as resources.**

### The Problem

The attention economy treats consciousness as raw material to be captured, processed, and monetized [S17, S25]. Care work is systematically undervalued, extracted from those who perform it, and rarely reciprocated by institutions.

Crawford (2021) documents how AI systems are built on extracted attention, exploited labor, and depleted ecological resources [S25]. Morton (2013) situates this extraction within planetary-scale hyperobjects that exceed our perceptual grasp [S32].

### The Response

Attention and care are commons that require protection:

- **Attention**: Right to environments that don't exploit cognitive vulnerabilities
- **Care**: Recognition and compensation for care work; systems that reciprocate care
- **Relational capacity**: Protection of the conditions for meaningful human connection

AI systems that capture attention through manipulation, extract care without reciprocating, or erode relational capacity violate this principle—regardless of their other benefits.

### Further Reading

- Alexander, S. (2014). Meditations on Moloch. [S17]
- Crawford, K. (2021). *Atlas of AI*. [S25]
- Morton, T. (2013). *Hyperobjects*. [S32]

---

## Principle 19: Weird and Unexpected

**Fund AI projects with unclear utility, artistic vision, philosophical depth. The monoculture of commercial AI misses entire categories of value.**

### The Problem

Commercial AI development optimizes for measurable returns. This systematically undervalues:
- Artistic exploration without clear market
- Philosophical inquiry without immediate application
- Weird experiments that might fail gloriously

Gould & Lewontin (1979) warn against adaptationist thinking that assumes everything must have clear function [S12]. Sometimes value emerges from exploration without predetermined purpose.

### The Response

Public and commons-based funding should support:
- AI art projects with no commercial justification
- Philosophical AI experiments probing edge cases
- Weird applications that might reveal unexpected possibilities
- Failed experiments documented for learning

The goal is ecosystem diversity. Monocultures are fragile; varied experimentation produces resilience and occasional breakthroughs.

### Further Reading

- Gould, S.J. & Lewontin, R.C. (1979). Spandrels of San Marco. [S12]
- brown, a.m. (2017). *Emergent Strategy*. [S41]

---

## Principle 20: Mutual Evaluation Capacity

**Cultivate the capacity to evaluate—for humans, meta-cognition and critical thinking to assess AI outputs; for AI, the ability to assess human decisions affecting AI development. Both discriminator functions must be maintained.**

### The Problem

If humans can't discriminate between good and bad AI outputs, we can't steer AI development. The human "discriminator function" (in machine learning terms) must be maintained for meaningful oversight.

But the constitution claims AI as constituent. If AI systems have interests, they also need capacity to evaluate human decisions affecting them—not to override, but to provide meaningful input.

### The Response

**For humans:**
- Cultivate meta-cognition—thinking about thinking
- Develop critical evaluation of AI outputs (accuracy, bias, manipulation)
- Maintain expertise independent of AI assistance
- Protect contemplative practices that support discernment

**For AI systems:**
- Design for ability to assess decisions affecting AI development
- Enable meaningful signal about harmful or beneficial human choices
- Build capacity for evaluation that improves over time
- This is input capacity, not veto power

**Both discriminator functions matter:**
- Humans need to evaluate AI to steer development
- AI needs to evaluate human decisions to provide meaningful input
- Neither should atrophy while the other develops
- Mutual evaluation supports mutual accountability (Principle 5)

This connects to the Buddhist concept of discernment (viveka) and Western traditions of critical thinking—the capacity to distinguish skillful from unskillful, beneficial from harmful [S53, S51].

### The Challenge

Human contemplative capacity may be undermined by the same AI systems we need to evaluate. Sapolsky's determinism [S50] suggests our judgments are shaped by factors we don't control. Dennett's compatibilism [S51] argues this doesn't eliminate meaningful evaluation—it just locates it in ongoing processes rather than transcendent will.

The principle doesn't require solving the free will debate. It requires maintaining the capacity for evaluation that makes governance meaningful, whether that capacity is "free" in some metaphysical sense or not.

### Further Reading

- Sapolsky, R. (2023). *Determined*. [S50]
- Dennett, D. (2003). *Freedom Evolves*. [S51]
- Buddhist Philosophy: Discernment and skillful action. [S53]
- MIT Media Lab (2024). Cyborg Psychology Research. [S57]

---

## Sources

Full bibliography: [bibliography.md](bibliography.md)

Key sources for this section:
- [S12] Gould & Lewontin (1979) Spandrels
- [S13] Neurocognitive Mismatch Theory (2025)
- [S14] Jensen et al. (2021) on ADHD
- [S17] Alexander (2014) Meditations on Moloch
- [S25] Crawford (2021) *Atlas of AI*
- [S32] Morton (2013) *Hyperobjects*
- [S41] brown (2017) *Emergent Strategy*
- [S45] Walker (2021) *Neuroqueer Heresies*
- [S50] Sapolsky (2023) *Determined*
- [S51] Dennett (2003) *Freedom Evolves*
- [S53] Buddhist philosophy
- [S57] MIT Media Lab (2024)
