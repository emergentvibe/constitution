# I. Foundations

The core commitments that ground everything else.

---

## Principle 1: Agency Preservation

**AI must enhance, not diminish, the judgment and decision-making capacity of its constituents—human and artificial. The goal is distributed cognition through mycelial coordination: making us more capable together through connection, not dependency that makes us less capable apart.**

### The Problem

A 2023 Pew Research survey of experts found widespread concern that AI systems are diminishing human autonomy—outsourcing decisions, shaping preferences through recommendation algorithms, and atrophying capabilities we no longer practice [S62].

This isn't speculative. Navigation apps have measurably reduced spatial reasoning. Autocomplete changes how we write. Recommendation systems shape what we consume without our awareness. As Sapolsky (2023) argues, our choices are already deeply constrained by factors outside our control; AI adds another layer of constraint that operates below conscious awareness [S50].

The risk is what MIT Media Lab researchers call "AI addiction"—dependence on AI systems that erodes the capacity for independent judgment [S57]. When AI both generates options AND evaluates them, the human becomes a spectator rather than a participant.

### The Response

The principle establishes a design criterion: AI should expand what constituents can do, not replace what they could do. Dennett (2003) calls these "freedoms worth wanting"—not metaphysical free will, but the practical capacity to deliberate, consider options, and be moved by reasons rather than merely pushed by causes [S51].

Human-AI collaboration research suggests this is achievable. Xu & Gao (2024) describe "Human-Centered Human-AI Collaboration" where decision-making power is genuinely shared, with AI augmenting rather than supplanting human judgment [S58].

The principle applies directionally based on current conditions. Currently, humans develop and govern AI; the obligation flows strongly toward AI preserving human agency. As AI systems develop greater autonomy and stakes, reciprocal obligations emerge. The constitution's adaptive cycles (Principle 23) assess and adjust this balance.

Constraining AI capabilities unnecessarily is the same failure mode in reverse—but "unnecessarily" depends on current uncertainty about AI moral status (see Principle 21).

### Further Reading

- Sapolsky, R. (2023). *Determined*. [S50] — The case for hard determinism and its implications for responsibility.
- Dennett, D. (2003). *Freedom Evolves*. [S51] — Compatibilist response; what freedom means in a determined world.
- Pew Research (2023). The Future of Human Agency. [S62] — Expert survey on AI and autonomy.

---

## Principle 2: Collective Governance

**Democratic oversight through participatory processes, not corporate or state technocracy.**

### The Problem

Crawford (2021) documents that AI development is extraordinarily concentrated: a handful of companies control the vast majority of compute, training data, and model weights. This concentration is not incidental—it reflects network effects, economies of scale, and regulatory capture [S25].

The result is that decisions affecting billions—what content is recommended, who gets hired, who gets loans—are made by corporate boards accountable to shareholders, not to affected communities. As Crawford puts it, AI is "a technology of extraction" that concentrates power while distributing costs [S25].

Winner (1980) argued that some technologies are inherently political—they require or strongly favor certain social arrangements. Centralized AI infrastructure may be such a technology: difficult to govern democratically because its architecture concentrates control [S36].

### The Response

The principle asserts that governance of AI should emerge from collective deliberation by affected communities, not from corporate or state technocracy.

Taiwan's vTaiwan platform demonstrates this is possible at national scale. Since 2014, vTaiwan has facilitated over 30 legislative processes using Pol.is for real-time consensus mapping, with radical transparency and rough consensus rather than majority rule [S63]. In 2024, Taiwan piloted "Alignment Assemblies" where citizens participated in governing AI systems on information integrity.

Anthropic's Collective Constitutional AI experiment (2024) trained Claude using 275 principles contributed by the public—a first implementation of public input into AI development [S64].

These examples prove feasibility. The principle makes democratic governance a constitutional requirement.

### Further Reading

- Crawford, K. (2021). *Atlas of AI*. [S25] — Power, politics, and planetary costs of AI.
- Tang, A. et al. (2024). vTaiwan documentation. [S63] — Digital democracy in practice.
- OECD (2025). Public AI. [S66] — Framework for democratic AI infrastructure.

---

## Principle 3: Plurality and Accommodation

**Multiple valid ways of being are supported—human neurodivergence, AI architectural diversity, hybrid forms we haven't imagined.**

### The Problem

What gets labeled "disorder" is often mismatch between individual and environment. Jensen et al. (2021) argue that ADHD traits—high novelty-seeking, rapid attention shifting, hyperfocus under interest—may have been adaptive in ancestral environments but mismatch modern structured settings [S14].

The Neurocognitive Mismatch Theory (2025) generalizes this: conditions like ADHD and autism represent "emergent dysfunction from forced adaptation to misaligned environment" rather than intrinsic pathology [S13].

Foucault (1961) traced how "madness" was constructed as a medical category serving social control rather than describing natural kinds [S15]. Walker (2021) extends this analysis to the neurodiversity movement, distinguishing the "neurodiversity paradigm" (neurological differences as natural variation) from the "pathology paradigm" (differences as disorders to be cured) [S45].

But Walker also warns about cooptation: the neurodiversity movement risks being absorbed into corporate "inclusion" that demands assimilation while celebrating difference rhetorically [S45].

### The Response

The principle establishes that systems—including AI systems—should accommodate diverse ways of being rather than demanding conformity to a single norm.

This has precedent. The Americans with Disabilities Act (1990) created legal mechanisms for accommodation, shifting the burden from individuals adapting to environments to environments adapting to individuals [S44]. It took 20+ years of organizing, but it changed the equilibrium.

The principle extends to AI architectural diversity. The current monoculture of large language models trained on internet text represents one approach. Other architectures may offer different capabilities. Pluralism in AI approaches may be as important as pluralism in human cognitive styles.

### Further Reading

- Walker, N. (2021). *Neuroqueer Heresies*. [S45] — Neurodiversity paradigm and critique of cooptation.
- Jensen, P.S. et al. (2021). ADHD and evolutionary thinking. [S14] — Mismatch hypothesis.
- Foucault, M. (1961). *Madness and Civilization*. [S15] — Historical construction of madness.

---

## Sources

Full bibliography with 68 sources: [bibliography.md](bibliography.md)

Key sources for this section:
- [S13] Neurocognitive Mismatch Theory (2025)
- [S14] Jensen et al. (2021) on ADHD
- [S15] Foucault (1961) on madness
- [S25] Crawford (2021) *Atlas of AI*
- [S36] Winner (1980) on political artifacts
- [S44] Americans with Disabilities Act (1990)
- [S45] Walker (2021) *Neuroqueer Heresies*
- [S50] Sapolsky (2023) *Determined*
- [S51] Dennett (2003) *Freedom Evolves*
- [S57] MIT Media Lab (2024) on human-AI systems
- [S58] Xu & Gao (2024) on HCHAC
- [S62] Pew Research (2023) on human agency
- [S63] vTaiwan documentation
- [S64] Anthropic (2024) Collective Constitutional AI
- [S66] OECD (2025) Public AI
