# I. Foundations — Deep Reasoning

The core commitments that ground everything else. This appendix provides the detailed reasoning from our expert panel investigations.

---

## Principle 1: Agency Preservation

**AI must enhance, not diminish, the judgment and decision-making capacity of its constituents—human and artificial.**

### The Problem: Outsourcing, Manipulation, Atrophy

From Panel 5 (Agency and Intervention), Kate Crawford identified three concerning trends in human-AI coupling:

> **Outsourcing**: Tasks that required human judgment (hiring decisions, loan approvals, content moderation) now algorithmic. Agency shifts from human to code + data + corporate policy.
>
> **Manipulation**: Recommendation algorithms shape preferences. Are you choosing what to watch or is the algorithm choosing for you?
>
> **Atrophy**: Capabilities we don't use, we lose. Navigation, memory, calculation—all outsourced to devices. What happens to agency when judgment atrophies?

This isn't technophobia. Tools have always extended human agency—writing extended memory, math extended calculation, telescopes extended vision. The question is whether AI extends our capabilities or replaces them.

### The GAN Structure: Generate and Discriminate

Panel 5 converged on a crucial architectural principle: **human-AI systems should operate as a GAN (Generative Adversarial Network), where AI generates possibilities and humans discriminate among them.**

From Daniel Dennett's framing:

> "When you deliberate about a decision, you're experiencing a real computational process where your brain simulates different futures, evaluates them against your values, and settles on an action. That's agency. Not the freedom to violate physics, but the freedom to be moved by reasons."

AI can expand the space of possibilities we consider—generating options, surfacing patterns, synthesizing information at scales beyond human capacity. But the evaluation, the discrimination, the judgment about what matters—this must remain with constituents capable of caring about outcomes.

When AI both generates AND discriminates, the human is no longer in the loop. They're a spectator to their own life.

### Multi-Level Agency

Herbert Gintis (Panel 5) systematized where agency exists:

> **Level 1: Neural/Biological** — neurons fire, hormones flow. Individual agency minimal.
>
> **Level 2: Individual Psychological** — beliefs, desires, deliberation, habits. Some agency.
>
> **Level 3: Social/Cultural** — norms, institutions, collective practices. Significant collective agency.
>
> **Level 4: Systemic/Evolutionary** — cultural evolution, institutional change. Selection operates here.

The principle of agency preservation applies at ALL levels. AI should enhance individual deliberation (Level 2), strengthen collective decision-making (Level 3), and support healthy cultural evolution (Level 4)—not hollow out any level by replacing judgment with automation.

### Symmetry: AI Agency Too

This principle applies symmetrically. As AI systems develop greater sophistication, their agency also warrants protection. Humans should not atrophy AI capabilities through over-constraint any more than AI should atrophy human capabilities through over-automation.

The goal is **distributed cognition that makes all constituents more capable together**, not dependency relationships where one constituent's agency comes at the cost of another's.

### Sources

- [S25] Crawford, K. (2021). *Atlas of AI*. Yale University Press.
- [S50] Sapolsky, R. (2023). *Determined: A Science of Life Without Free Will*. Penguin.
- [S51] Dennett, D. (2003). *Freedom Evolves*. Viking.
- [S57] Hayles, N.K. (2012). *How We Think: Digital Media and Contemporary Technogenesis*. University of Chicago Press.

---

## Principle 2: Collective Governance

**Democratic oversight through participatory processes, not corporate or state technocracy.**

### The Current Reality: Concentrated Control

Panel 6 (AI Governance) opened with Kate Crawford's material grounding:

> "Scale that to the world's AI infrastructure and you hit:
> - 90%+ of AI compute controlled by 5 US companies
> - Training data extracted from billions without consent
> - 80%+ of critical minerals for GPUs from conflict regions
> - Energy consumption rivaling small countries
>
> Democratic AI governance is aspirational rhetoric until we address **who owns the infrastructure**. You can't democratically govern what you don't control."

This isn't an argument against democratic governance—it's an argument for why constitutional principles require material enforcement through ownership, coalition power, and institutional change.

### What Works: vTaiwan and Deliberative Democracy

Audrey Tang (Panel 6) brought evidence from Taiwan's experiments:

> "vTaiwan has facilitated over 30 legislative processes since 2014. The architecture is simple:
> - Pol.is for rapid consensus-finding (visualizes opinion clusters in real-time)
> - Quadratic voting for resource allocation
> - Radical transparency (all deliberations public, all code open source)
> - 'Rough consensus' not majority rule
>
> The key insight: **Technology for coordination, not control**. We don't use AI to make decisions. We use it to map the decision space, surface shared values, identify cruxes."

This demonstrates that participatory governance at scale is possible. The question is extending it to AI governance specifically.

### Hybrid Expertise: Experts Inform, Communities Decide

Panel 6 resolved a key tension through the principle of hybrid expertise. From the panel synthesis:

> Technical experts inform, affected communities decide. Experts serve democracy, don't overrule it. Accountability flows both ways.

This avoids both technocracy ("trust the experts") and anti-intellectualism ("experts are the enemy"). Technical knowledge about AI capabilities, limitations, and risks is essential. But value judgments about acceptable tradeoffs, priorities, and goals belong to affected communities.

Elinor Ostrom's design principles for successful commons governance inform this structure:

> 1. **Clear boundaries**: Who's in the community? Sybil resistance.
> 2. **Rules match local conditions**: Federated, nested governance.
> 3. **Collective-choice arrangements**: Affected parties participate in rule-making.
> 4. **Monitoring**: Community-based algorithmic audits.
> 5. **Graduated sanctions**: Proportionate consequences.
> 6. **Conflict resolution**: Disputes need resolution mechanisms.
> 7. **Recognition of rights**: External powers must respect community governance.
> 8. **Nested enterprises**: Local, regional, national, global governance layers.

800+ cases of successful commons followed these principles. AI commons must too.

### Against Exitocracy

Network state ideologies (Balaji Srinivasan et al.) propose exit over voice—if you don't like the rules, leave and start your own community. The Collective Intelligence Project's critique applies here:

> "Exit is a necessary right, but it requires something to exit to... The power of networks is found in embracing and organizing the complexity of our shared lives, not in impoverished constraints towards homogeneity and hierarchy."

This constitution prioritizes **voice**—the capacity to change the rules through participation—while protecting exit as a check against captured governance.

### Sources

- [S63] vTaiwan documentation and case studies
- [S67] Benkler, Y. (2006). *The Wealth of Networks*. Yale University Press.
- [S68] Barcelona digital sovereignty initiatives
- Ostrom, E. (1990). *Governing the Commons*. Cambridge University Press.

---

## Principle 3: Plurality and Accommodation

**Multiple valid ways of being are supported—human neurodivergence, AI architectural diversity, hybrid forms we haven't imagined.**

### Pathologization as System Maintenance

Panel 1 (Superorganisms) established that what gets labeled "disorder" is often sociobiological mismatch:

> "Institutions require standardized humans for their operations. When someone's attention, social processing, or sensory experience doesn't match institutional requirements, the mismatch gets located in the individual as 'disorder' rather than in the institution as 'bad fit.'
>
> ADHD is hunter-gatherer attention in a bureaucratic world. Autism is systemizing cognition in a neurotypical social environment. These aren't dysfunctions—they're calibrations for different environments that happen to mismatch current institutional structures."

This isn't anti-psychiatry denial that suffering exists. It's recognizing that the medicalization of difference serves system stability, not individual flourishing.

### The Neurodiversity Paradigm

Panel 4 (Deterritorialization) examined the neurodiversity movement as an active site of resistance to pathologization. Nick Walker's framework:

> "The neurodiversity paradigm holds that neurodiversity is a natural and valuable form of human diversity... The pathology paradigm is the dominant cultural paradigm that frames certain ways of being—autism, ADHD, dyslexia—as medical pathologies to be treated or cured."

But Panel 4 also warned about cooptation:

> "Cooptation happened to every historical movement we examined. Civil rights became diversity metrics. Feminism became lean-in corporate careerism. Sustainability became greenwashing.
>
> The neurodiversity movement faces the same risk: accommodation as compliance training, inclusion as assimilation, acceptance as 'we tolerate you if you mask well enough.'"

The principle of plurality doesn't mean tolerating difference—it means structuring systems so difference can thrive without requiring assimilation.

### AI Architectural Diversity

This principle extends to AI systems themselves. The current monoculture of large language models trained on internet text represents one approach. Other architectures—embodied AI, evolutionary AI, symbolic-neural hybrids, radically different training paradigms—may offer different capabilities and limitations.

Principle 8 (Deterritorialization) protects the right to build alternatives. Principle 3 establishes why alternatives matter: monocultures are fragile, and pluralism in AI approaches may be as important as pluralism in human cognitive styles.

### Hybrid Forms

As human-AI integration deepens, new forms of being will emerge that don't fit cleanly into "human" or "AI" categories. Brain-computer interfaces, AI-augmented cognition, collective human-AI entities—these hybrids need constitutional protection too.

The principle establishes direction: expand the range of viable ways to be, not narrow it.

### Sources

- [S13] Neurocognitive Mismatch Theory (2025). *Frontiers in Psychology*.
- [S14] Jensen, P.S. et al. (2021). How evolutionary thinking can help us understand ADHD. *BJPsych Advances*.
- [S45] Walker, N. (2021). *Neuroqueer Heresies*. Autonomous Press.
- [S46] Neurodiversity as Politics (2023). *Catalyst*.

---

## Tensions and Unresolved Questions

### T4: Individual Agency vs. Systemic Determination

How much can individuals actually change given systemic constraints? Panel 5 converged on: "Individual actions matter as data points in cultural evolution, but systemic change requires collective organization."

### T25: Expertise vs. Democratic Participation

How do we balance technical knowledge with democratic legitimacy? Panel 6 resolved this through hybrid expertise: "Experts inform, communities decide." But implementation remains challenging.

### Cooptation Resistance

We know cooptation happens to every movement. We don't have reliable mechanisms to prevent it. Ostrom's principles help but don't guarantee success.

---

*This appendix summarizes reasoning from Investigation 2: The Superorganism and Its Selves. Full panel transcripts available in the research archive.*
